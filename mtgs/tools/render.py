#-------------------------------------------------------------------------------#
# MTGS: Multi-Traversal Gaussian Splatting (https://arxiv.org/abs/2503.12552)   #
# Source code: https://github.com/OpenDriveLab/MTGS                             #
# Copyright (c) OpenDriveLab. All rights reserved.                              #
#-------------------------------------------------------------------------------#

#!/usr/bin/env python
from __future__ import annotations

import gzip
import json
import os
import pickle
import sys
from contextlib import ExitStack, contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Union, Tuple, Callable

import mediapy as media
import numpy as np
import torch
import tyro
from jaxtyping import Float
from pyquaternion import Quaternion
from rich import box, style
from rich.panel import Panel
from rich.progress import BarColumn, Progress, TaskProgressColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table
from torch import Tensor
from torch.nn import functional as F
from typing_extensions import Annotated
import yaml

from nerfstudio.cameras.camera_paths import get_interpolated_poses_many, get_path_from_json
from nerfstudio.cameras.cameras import Cameras, CameraType
from nerfstudio.data.datamanagers.base_datamanager import VanillaDataManager, VanillaDataManagerConfig
from nerfstudio.data.datamanagers.full_images_datamanager import FullImageDatamanagerConfig
from nerfstudio.data.datasets.base_dataset import Dataset
from nerfstudio.data.scene_box import OrientedBox
from nerfstudio.data.utils.dataloaders import FixedIndicesEvalDataloader
from nerfstudio.engine.trainer import TrainerConfig
from nerfstudio.model_components import renderers
from nerfstudio.pipelines.base_pipeline import Pipeline
from nerfstudio.utils import colormaps, install_checks
from nerfstudio.utils.rich_utils import CONSOLE, ItersPerSecColumn
from nuplan_scripts.utils.config import load_config
from nuplan_scripts.utils.video_scene_dict_tools import VideoScene


def _render_trajectory_video(
    pipeline: Pipeline,
    cameras: Cameras,
    output_filename: Path,
    rendered_output_names: List[str],
    crop_data: Optional[CropData] = None,
    rendered_resolution_scaling_factor: float = 1.0,
    seconds: float = 5.0,
    output_format: Literal["images", "video", "none"] = "video",
    image_format: Literal["jpeg", "png"] = "jpeg",
    jpeg_quality: int = 100,
    depth_near_plane: Optional[float] = None,
    depth_far_plane: Optional[float] = None,
    colormap_options: colormaps.ColormapOptions = colormaps.ColormapOptions(),
    filenames: Optional[List[str]] = None,
    per_camera_metadata: Optional[List[Dict[str, Any]]] = None,
) -> None:
    """Helper function to create a video of the spiral trajectory.

    Args:
        pipeline: Pipeline to evaluate with.
        cameras: Cameras to render.
        output_filename: Name of the output file.
        rendered_output_names: List of outputs to visualise.
        crop_data: Crop data to apply to the rendered images.
        rendered_resolution_scaling_factor: Scaling factor to apply to the camera image resolution.
        seconds: Length of output video.
        output_format: How to save output data.
        depth_near_plane: Closest depth to consider when using the colormap for depth. If None, use min value.
        depth_far_plane: Furthest depth to consider when using the colormap for depth. If None, use max value.
        colormap_options: Options for colormap.
    """
    CONSOLE.print("[bold green]Creating trajectory " + output_format)
    cameras.rescale_output_resolution(rendered_resolution_scaling_factor)
    cameras = cameras.to(pipeline.device)
    fps = len(cameras) / seconds

    progress = Progress(
        TextColumn(":movie_camera: Rendering :movie_camera:"),
        BarColumn(),
        TaskProgressColumn(
            text_format="[progress.percentage]{task.completed}/{task.total:>.0f}({task.percentage:>3.1f}%)",
            show_speed=True,
        ),
        ItersPerSecColumn(suffix="fps"),
        TimeRemainingColumn(elapsed_when_finished=False, compact=False),
        TimeElapsedColumn(),
    )
    output_image_dir = output_filename.parent / output_filename.stem
    if "images" in output_format:
        output_image_dir.mkdir(parents=True, exist_ok=True)
    if "video" in output_format:
        # make the folder if it doesn't exist
        output_filename.parent.mkdir(parents=True, exist_ok=True)
        # NOTE:
        # we could use ffmpeg_args "-movflags faststart" for progressive download,
        # which would force moov atom into known position before mdat,
        # but then we would have to move all of mdat to insert metadata atom
        # (unless we reserve enough space to overwrite with our uuid tag,
        # but we don't know how big the video file will be, so it's not certain!)
    if filenames is not None:
        assert len(filenames) == len(cameras), "filenames must have the same length as cameras"
    if per_camera_metadata is not None:
        assert len(per_camera_metadata) == len(cameras), "per_camera_metadata must have the same length as cameras"

    with ExitStack() as stack:
        writer = None
        render_images = []
        with progress:
            for camera_idx in progress.track(range(cameras.size), description=""):
                obb_box = None
                if crop_data is not None:
                    obb_box = crop_data.obb

                camera = cameras[camera_idx : camera_idx + 1]
                if per_camera_metadata is not None:
                    if camera.metadata is None:
                        camera.metadata = {}
                    camera.metadata.update(per_camera_metadata[camera_idx])
                if crop_data is not None:
                    with renderers.background_color_override_context(
                        crop_data.background_color.to(pipeline.device)
                    ), torch.no_grad():
                        outputs = pipeline.model.get_outputs_for_camera(
                            camera, obb_box=obb_box
                        )
                else:
                    with torch.no_grad():
                        outputs = pipeline.model.get_outputs_for_camera(
                            camera, obb_box=obb_box
                        )

                render_image = []
                for rendered_output_name in rendered_output_names:
                    if rendered_output_name not in outputs:
                        CONSOLE.rule("Error", style="red")
                        CONSOLE.print(f"Could not find {rendered_output_name} in the model outputs", justify="center")
                        CONSOLE.print(
                            f"Please set --rendered_output_name to one of: {outputs.keys()}", justify="center"
                        )
                        sys.exit(1)
                    output_image = outputs[rendered_output_name]
                    is_depth = rendered_output_name.find("depth") != -1
                    if is_depth:
                        output_image = (
                            colormaps.apply_depth_colormap(
                                output_image,
                                accumulation=outputs["accumulation"],
                                near_plane=depth_near_plane,
                                far_plane=depth_far_plane,
                                colormap_options=colormap_options,
                            )
                            .cpu()
                            .numpy()
                        )
                    else:
                        output_image = (
                            colormaps.apply_colormap(
                                image=output_image,
                                colormap_options=colormap_options,
                            )
                            .cpu()
                            .numpy()
                        )
                    render_image.append(output_image)

                render_image = np.concatenate(render_image, axis=1)
                render_images.append(render_image)
                if "images" in output_format:
                    img_filename = filenames[camera_idx] if filenames is not None else f"{camera_idx:05d}"
                    if image_format == "png":
                        media.write_image(output_image_dir / f"{img_filename}.png", render_image, fmt="png")
                    if image_format == "jpeg":
                        media.write_image(
                            output_image_dir / f"{img_filename}.jpg", render_image, fmt="jpeg", quality=jpeg_quality
                        )
                if "video" in output_format:
                    if writer is None:
                        render_width = int(render_image.shape[1])
                        render_height = int(render_image.shape[0])
                        writer = stack.enter_context(
                            media.VideoWriter(
                                path=output_filename,
                                shape=(render_height, render_width),
                                fps=fps,
                            )
                        )
                    writer.add_image(render_image)

    table = Table(
        title=None,
        show_header=False,
        box=box.MINIMAL,
        title_style=style.Style(bold=True),
    )
    if output_format == "video":
        table.add_row("Video", str(output_filename))
    elif output_format == "images":
        table.add_row("Images", str(output_image_dir))
    if output_format != "none":
        CONSOLE.print(Panel(table, title="[bold][green]:tada: Render Complete :tada:[/bold]", expand=False))

    return render_images


def _resolve_video_scene_pkl(config: TrainerConfig, override: Optional[Path]) -> Optional[Path]:
    if override is not None:
        return override
    try:
        dataparser_cfg = config.pipeline.datamanager.dataparser
        road_block_cfg = load_config(Path(dataparser_cfg.road_block_config).as_posix())
        video_scene = VideoScene(road_block_cfg)
        return Path(video_scene.pickle_path)
    except Exception as exc:
        CONSOLE.print(f"Could not resolve video_scene_dict.pkl: {exc}", style="yellow")
        return None


def _load_frame_info_map(video_scene_pkl: Path, travel_id: int) -> Dict[str, Dict[str, Any]]:
    with open(video_scene_pkl, "rb") as f:
        video_scene_dict = pickle.load(f)
    matches = [k for k in video_scene_dict.keys() if k.endswith(f"-{travel_id}")]
    if not matches:
        raise ValueError(f"No video_token found for travel_id={travel_id} in {video_scene_pkl}")
    if len(matches) > 1:
        CONSOLE.print(f"Multiple video_tokens match travel_id={travel_id}, using {matches[0]}", style="yellow")
    frame_infos = video_scene_dict[matches[0]]["frame_infos"]
    return {info["token"]: info for info in frame_infos}


def _default_sticker_layout() -> List[List[Optional[str]]]:
    return [
        ["CAM_L0", "CAM_F0", "CAM_R0"],
        ["CAM_L1", None, "CAM_R1"],
        ["CAM_L2", "CAM_B0", "CAM_R2"],
    ]


def _resolve_sticker_layout(
    camera_names: List[str],
    layout_rows: List[List[Optional[str]]],
) -> Tuple[Dict[str, Tuple[int, int]], int, int]:
    rows = len(layout_rows)
    cols = max((len(row) for row in layout_rows), default=0)
    positions: Dict[str, Tuple[int, int]] = {}
    for row_idx, row in enumerate(layout_rows):
        for col_idx, name in enumerate(row):
            if name is None:
                continue
            if name in camera_names:
                positions[name] = (row_idx, col_idx)

    unknown = [name for name in camera_names if name not in positions]
    if unknown:
        CONSOLE.print(f"Placing unknown cameras at bottom: {unknown}", style="yellow")
        base_row = rows
        for idx, name in enumerate(unknown):
            positions[name] = (base_row + idx // cols, idx % cols)
        rows += (len(unknown) + cols - 1) // cols
    return positions, rows, cols


def _resize_image(image: np.ndarray, scale: float) -> np.ndarray:
    if scale == 1.0:
        return image
    if scale <= 0.0:
        raise ValueError("sticker_scale must be > 0")
    tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
    if not torch.is_floating_point(tensor):
        tensor = tensor.float()
    tensor = F.interpolate(tensor, scale_factor=scale, mode="bilinear", align_corners=False)
    return tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()


def _compose_sticker_video(
    rendered: Dict[str, List[np.ndarray]],
    sticker_scale: float,
    padding: int,
    background_color: Tuple[int, int, int],
    layout_rows: Optional[List[List[Optional[str]]]] = None,
) -> List[np.ndarray]:
    if not rendered:
        raise ValueError("No rendered frames provided for sticker composition.")

    layout_rows = layout_rows or _default_sticker_layout()
    camera_names = list(rendered.keys())
    positions, rows, cols = _resolve_sticker_layout(camera_names, layout_rows)
    if cols <= 0 or rows <= 0:
        raise ValueError("Sticker layout must have at least one cell.")
    if padding < 0:
        raise ValueError("padding must be >= 0")

    frame_counts = [len(frames) for frames in rendered.values()]
    num_frames = min(frame_counts)
    if num_frames == 0:
        raise ValueError("Rendered frames are empty.")
    if len(set(frame_counts)) != 1:
        CONSOLE.print("Mismatched frame counts across cameras; using shortest sequence.", style="yellow")

    sample_frame = next(iter(rendered.values()))[0]
    sample_sizes = [rendered[name][0].shape[:2] for name in positions.keys()]
    max_h = max(h for h, _ in sample_sizes)
    max_w = max(w for _, w in sample_sizes)
    cell_h = int(round(max_h * sticker_scale))
    cell_w = int(round(max_w * sticker_scale))
    if cell_h <= 0 or cell_w <= 0:
        raise ValueError("sticker_scale results in non-positive cell size.")

    canvas_h = rows * cell_h + (rows + 1) * padding
    canvas_w = cols * cell_w + (cols + 1) * padding

    if np.issubdtype(sample_frame.dtype, np.floating):
        bg = np.array(background_color, dtype=sample_frame.dtype) / 255.0
    else:
        bg = np.array(background_color, dtype=sample_frame.dtype)

    composed = []
    for frame_idx in range(num_frames):
        canvas = np.empty((canvas_h, canvas_w, 3), dtype=sample_frame.dtype)
        canvas[:, :] = bg
        for cam_name, (row_idx, col_idx) in positions.items():
            frame = rendered[cam_name][frame_idx]
            frame = _resize_image(frame, sticker_scale)
            h, w = frame.shape[:2]
            y0 = padding + row_idx * (cell_h + padding) + (cell_h - h) // 2
            x0 = padding + col_idx * (cell_w + padding) + (cell_w - w) // 2
            canvas[y0 : y0 + h, x0 : x0 + w] = frame
        composed.append(canvas)

    return composed


def _apply_camera_offset_camera_coords(
    camera_to_worlds: torch.Tensor,
    offset_cam: Tuple[float, float, float],
) -> torch.Tensor:
    if offset_cam == (0.0, 0.0, 0.0):
        return camera_to_worlds
    offset = torch.tensor(offset_cam, dtype=camera_to_worlds.dtype, device=camera_to_worlds.device)
    offset_world = torch.einsum("bij,j->bi", camera_to_worlds[..., :3, :3], offset)
    updated = camera_to_worlds.clone()
    updated[..., :3, 3] = updated[..., :3, 3] + offset_world
    return updated

def find_ckpt_path(config: TrainerConfig) -> Tuple[Path, int]:
    assert config.load_dir is not None
    if config.load_step is None:
        CONSOLE.print("Loading latest checkpoint from load_dir")
        # NOTE: this is specific to the checkpoint name format
        if not os.path.exists(config.load_dir):
            CONSOLE.rule("Error", style="red")
            CONSOLE.print(f"No checkpoint directory found at {config.load_dir}, ", justify="center")
            CONSOLE.print(
                "Please make sure the checkpoint exists, they should be generated periodically during training",
                justify="center",
            )
            sys.exit(1)
        load_step = sorted(int(x[x.find("-") + 1 : x.find(".")]) for x in os.listdir(config.load_dir))[-1]
    else:
        load_step = config.load_step
    load_path = config.load_dir / f"step-{load_step:09d}.ckpt"
    assert load_path.exists(), f"Checkpoint {load_path} does not exist"
    return load_path

def eval_setup(
    config_path: Path,
    eval_num_rays_per_chunk: Optional[int] = None,
    test_mode: Literal["test", "val", "inference"] = "test",
    update_config_callback: Optional[Callable[[TrainerConfig], TrainerConfig]] = None,
) -> Tuple[TrainerConfig, Pipeline, Path, int]:
    # load save config
    config = yaml.load(config_path.read_text(), Loader=yaml.Loader)
    assert isinstance(config, TrainerConfig)

    # config.pipeline.datamanager._target = all_methods[config.method_name].pipeline.datamanager._target
    if eval_num_rays_per_chunk:
        config.pipeline.model.eval_num_rays_per_chunk = eval_num_rays_per_chunk

    # load checkpoints from wherever they were saved
    config.load_dir = config.get_checkpoint_dir()
    checkpoint_path = find_ckpt_path(config)

    if 'camera_optimizer' in config.pipeline.model.__dict__ \
        and config.pipeline.model.camera_optimizer.mode == "SO3xR3":
        config.pipeline.datamanager.dataparser.load_cam_optim_from = checkpoint_path
    config.pipeline.datamanager.dataparser.eval_2hz = False

    if update_config_callback is not None:
        config = update_config_callback(config)


    # setup pipeline (which includes the DataManager)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pipeline = config.pipeline.setup(device=device, test_mode=test_mode)
    assert isinstance(pipeline, Pipeline)
    pipeline.eval()

    # load checkpointed information
    loaded_state = torch.load(checkpoint_path, map_location="cpu")
    pipeline.load_pipeline(loaded_state["pipeline"], loaded_state["step"])

    return config, pipeline, checkpoint_path, loaded_state["step"]

def _get_interpolated_camera_path(cameras: Cameras, steps: int, order_poses: bool) -> Cameras:
    """Generate a camera path between two cameras. Uses the camera type of the first camera

    Args:
        cameras: Cameras object containing intrinsics of all cameras.
        steps: The number of steps to interpolate between the two cameras.

    Returns:
        A new set of cameras along a path.
    """
    Ks = cameras.get_intrinsics_matrices()
    poses = cameras.camera_to_worlds
    poses, Ks = get_interpolated_poses_many(poses, Ks, steps_per_transition=steps+1, order_poses=order_poses)
    camera_timestamps = cameras.times.squeeze()

    timestamps_new = []
    for i in range(len(camera_timestamps) - 1):
        timestamps_new.append(
            torch.linspace(camera_timestamps[i], camera_timestamps[i+1], steps+1, dtype=torch.float)[:-1])
    timestamps_new.append(camera_timestamps[-1][None])
    timestamps_new = torch.cat(timestamps_new, dim=0)

    mask = torch.cat([torch.ones(steps, dtype=torch.bool), torch.zeros(1, dtype=torch.bool)])
    mask = mask.repeat(len(cameras) - 1)
    mask[-1] = True

    poses = poses[mask]
    Ks = Ks[mask]

    cameras = Cameras(
        fx=Ks[:, 0, 0],
        fy=Ks[:, 1, 1],
        cx=Ks[0, 0, 2],
        cy=Ks[0, 1, 2],
        camera_type=cameras.camera_type[0],
        camera_to_worlds=poses,
        times=timestamps_new,
        width=cameras.width[0],
        height=cameras.height[0],
    )
    return cameras

@dataclass
class CropData:
    """Data for cropping an image."""

    background_color: Float[Tensor, "3"] = torch.Tensor([0.0, 0.0, 0.0])
    """background color"""
    obb: OrientedBox = field(default_factory=lambda: OrientedBox(R=torch.eye(3), T=torch.zeros(3), S=torch.ones(3) * 2))
    """Oriented box representing the crop region"""

    # properties for backwards-compatibility interface
    @property
    def center(self):
        return self.obb.T

    @property
    def scale(self):
        return self.obb.S


def get_crop_from_json(camera_json: Dict[str, Any]) -> Optional[CropData]:
    """Load crop data from a camera path JSON

    args:
        camera_json: camera path data
    returns:
        Crop data
    """
    if "crop" not in camera_json or camera_json["crop"] is None:
        return None
    bg_color = camera_json["crop"]["crop_bg_color"]
    center = camera_json["crop"]["crop_center"]
    scale = camera_json["crop"]["crop_scale"]
    rot = (0.0, 0.0, 0.0) if "crop_rot" not in camera_json["crop"] else tuple(camera_json["crop"]["crop_rot"])
    assert len(center) == 3
    assert len(scale) == 3
    assert len(rot) == 3
    return CropData(
        background_color=torch.Tensor([bg_color["r"] / 255.0, bg_color["g"] / 255.0, bg_color["b"] / 255.0]),
        obb=OrientedBox.from_params(center, rot, scale),
    )


@dataclass
class BaseRender:
    """Base class for rendering."""

    load_config: Path
    """Path to config YAML file."""
    output_path: Path = Path("renders/output.mp4")
    """Path to output video file."""
    image_format: Literal["jpeg", "png"] = "jpeg"
    """Image format"""
    jpeg_quality: int = 100
    """JPEG quality"""
    downscale_factor: float = 2.0
    """Scaling factor to apply to the camera image resolution."""
    eval_num_rays_per_chunk: Optional[int] = None
    """Specifies number of rays per chunk during eval. If None, use the value in the config file."""
    rendered_output_names: List[str] = field(default_factory=lambda: ["rgb"])
    """Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis"""
    depth_near_plane: Optional[float] = None
    """Closest depth to consider when using the colormap for depth. If None, use min value."""
    depth_far_plane: Optional[float] = None
    """Furthest depth to consider when using the colormap for depth. If None, use max value."""
    colormap_options: colormaps.ColormapOptions = colormaps.ColormapOptions()
    """Colormap options."""
    multi_view_camera: Tuple[Literal['CAM_F0', 'CAM_L0', 'CAM_R0', 'CAM_L1', 'CAM_R1', 'CAM_L2', 'CAM_R2', 'CAM_B0'], ...] = \
        ('CAM_F0', 'CAM_L0', 'CAM_R0', 'CAM_L1', 'CAM_R1', 'CAM_L2', 'CAM_R2', 'CAM_B0')


@dataclass
class RenderCameraPath(BaseRender):
    """Render a camera path generated by the viewer or blender add-on."""

    camera_path_filename: Path = Path("camera_path.json")
    """Filename of the camera path to render."""
    output_format: Literal["images", "video"] = "video"
    """How to save output data."""

    def main(self) -> None:
        """Main function."""
        _, pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,
            test_mode="inference",
        )

        install_checks.check_ffmpeg_installed()

        with open(self.camera_path_filename, "r", encoding="utf-8") as f:
            camera_path = json.load(f)
        seconds = camera_path["seconds"]
        crop_data = get_crop_from_json(camera_path)
        camera_path = get_path_from_json(camera_path)

        if (
            camera_path.camera_type[0] == CameraType.OMNIDIRECTIONALSTEREO_L.value
            or camera_path.camera_type[0] == CameraType.VR180_L.value
        ):
            # temp folder for writing left and right view renders
            temp_folder_path = self.output_path.parent / (self.output_path.stem + "_temp")

            Path(temp_folder_path).mkdir(parents=True, exist_ok=True)
            left_eye_path = temp_folder_path / "render_left.mp4"

            self.output_path = left_eye_path

            if camera_path.camera_type[0] == CameraType.OMNIDIRECTIONALSTEREO_L.value:
                CONSOLE.print("[bold green]:goggles: Omni-directional Stereo VR :goggles:")
            else:
                CONSOLE.print("[bold green]:goggles: VR180 :goggles:")

            CONSOLE.print("Rendering left eye view")

        # add mp4 suffix to video output if none is specified
        if self.output_format == "video" and str(self.output_path.suffix) == "":
            self.output_path = self.output_path.with_suffix(".mp4")

        _render_trajectory_video(
            pipeline,
            camera_path,
            output_filename=self.output_path,
            rendered_output_names=self.rendered_output_names,
            rendered_resolution_scaling_factor=1.0 / self.downscale_factor,
            crop_data=crop_data,
            seconds=seconds,
            output_format=self.output_format,
            image_format=self.image_format,
            jpeg_quality=self.jpeg_quality,
            depth_near_plane=self.depth_near_plane,
            depth_far_plane=self.depth_far_plane,
            colormap_options=self.colormap_options
        )


@dataclass
class RenderInterpolated(BaseRender):
    """Render a trajectory that interpolates between training or eval dataset images."""

    pose_source: Literal["eval", "train"] = "eval"
    """Pose source to render."""
    output_path: Optional[Path] = None
    """Path to output video for multi-traversal"""
    interpolation_steps: int = 6
    """Number of interpolation steps between eval dataset cameras."""
    order_poses: bool = False
    """Whether to order camera poses by proximity."""
    frame_rate: int = 60
    """Frame rate of the output video."""
    output_format: Literal["images", "video"] = "video"
    """How to save output data."""

    def main(self) -> None:
        """Main function."""
        def update_config(config: TrainerConfig) -> TrainerConfig:
            config.pipeline.datamanager.dataparser.train_split_fraction = 1.0
            config.pipeline.datamanager.dataparser.cameras = self.multi_view_camera
            CONSOLE.print(f'Using cams {self.multi_view_camera}.', style="bold green")
            return config

        config, pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,
            test_mode="inference",
            update_config_callback = update_config
        )

        install_checks.check_ffmpeg_installed()

        if self.pose_source == "eval":
            assert pipeline.datamanager.eval_dataset is not None
            cameras = pipeline.datamanager.eval_dataset.cameras
            dataparser_outputs = pipeline.datamanager.eval_dataset._dataparser_outputs
        else:
            assert pipeline.datamanager.train_dataset is not None
            cameras = pipeline.datamanager.train_dataset.cameras
            dataparser_outputs = pipeline.datamanager.train_dataset._dataparser_outputs

        num_cameras = len(self.multi_view_camera)

        def render_travel(output_path, cameras, num_cameras, travel_id=None):
            cam_split = []
            for i in range(num_cameras):
                cam_split.append(cameras[i::num_cameras])

            rendered = {}
            for cam_id, cameras in enumerate(cam_split):
                cam_name = self.multi_view_camera[cam_id]
                seconds = (self.interpolation_steps * (len(cameras) - 1) + 1) / self.frame_rate
                camera_path = _get_interpolated_camera_path(
                    cameras=cameras,
                    steps=self.interpolation_steps,
                    order_poses=self.order_poses
                )
                # the metadata will be kept after slicing the camera path
                camera_path.metadata = {'travel_id': travel_id}
                assert self.output_format == "video"
                output_filename = output_path / f"{cam_name}.mp4"

                images = _render_trajectory_video(
                    pipeline,
                    camera_path,
                    output_filename=Path(output_filename),
                    rendered_output_names=self.rendered_output_names,
                    rendered_resolution_scaling_factor=1.0 / self.downscale_factor,
                    seconds=seconds,
                    output_format=self.output_format,
                    image_format=self.image_format,
                    depth_near_plane=self.depth_near_plane,
                    depth_far_plane=self.depth_far_plane,
                    colormap_options=self.colormap_options
                )
                rendered[cam_name] = images

            # concat and save videos
            videos = []
            for cam in ('CAM_L0', 'CAM_F0', 'CAM_R0'):
                video = rendered[cam]
                videos.append(video)
            videos = np.concatenate(videos, axis=-2)
            media.write_video(output_path / "concat_front.mp4", videos, fps=self.frame_rate)

            videos = []
            for cam in ('CAM_R2', 'CAM_B0', 'CAM_L2'):
                video = rendered[cam]
                videos.append(video)
            videos = np.concatenate(videos, axis=-2)
            media.write_video(output_path / "concat_back.mp4", videos, fps=self.frame_rate)

        base_output_path = Path()
        if self.output_path is not None:
            base_output_path = self.output_path
        else:
            if hasattr(config, "base_dir"):
                base_output_path = Path(f"renders/scene_videos/{os.path.basename(config.base_dir)}")
            else:
                base_output_path = Path(f"renders/{config.experiment_name}")

        if hasattr(dataparser_outputs, "travel_ids") and dataparser_outputs.travel_ids is not None:
            travel_ids = dataparser_outputs.travel_ids
            travel_id_set = list(set(travel_ids))
            cameras_travels = {k:[] for k in travel_id_set}
            for idx, travel_id in enumerate(travel_ids):
                cameras_travels[travel_id].append(idx)
            for travel_id in travel_id_set:
                output_path = base_output_path / f"travel_{travel_id}"
                travel_indices = torch.tensor(cameras_travels[travel_id], dtype=torch.int64)
                render_travel(output_path, cameras[travel_indices], num_cameras, travel_id)
        else:
            render_travel(base_output_path, cameras, num_cameras)


@dataclass
class RenderSticker(BaseRender):
    """Render a fixed-layout multi-view sticker video on a blank background."""

    pose_source: Literal["eval", "train"] = "eval"
    """Pose source to render."""
    output_path: Optional[Path] = None
    """Base output directory."""
    interpolation_steps: int = 6
    """Number of interpolation steps between eval dataset cameras. Use 1 for no interpolation."""
    order_poses: bool = False
    """Whether to order camera poses by proximity."""
    frame_rate: int = 60
    """Frame rate of the output video."""
    output_name: str = "sticker.mp4"
    """Output file name for the composed video."""
    sticker_scale: float = 0.5
    """Scale factor for each sticker view."""
    padding: int = 10
    """Padding (pixels) between stickers and canvas edge."""
    background_color: Tuple[int, int, int] = (255, 255, 255)
    """Background color as RGB 0-255."""
    camera_offset_cam: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    """Camera offset in camera coordinates (x, y, z)."""
    retarget_vehicle_token: Optional[str] = None
    """Track token of the vehicle to retarget during rendering."""
    retarget_video_scene_pkl: Optional[Path] = None
    """Override path to video_scene_dict.pkl for retargeting."""
    retarget_forward: float = 10.0
    """Forward offset in ego coordinates (meters)."""
    retarget_lateral: float = 0.0
    """Lateral offset in ego coordinates (meters). Positive is left."""
    retarget_up: float = 0.0
    """Vertical offset in ego coordinates (meters)."""
    retarget_yaw_deg: float = 90.0
    """Yaw offset in degrees relative to ego heading."""
    retarget_frame_start: Optional[int] = None
    """Start frame index (inclusive) for retargeting."""
    retarget_frame_end: Optional[int] = None
    """End frame index (inclusive) for retargeting."""
    ego_offset_forward: float = 0.0
    """Ego forward offset in meters (applied to all cameras)."""
    ego_offset_lateral: float = 0.0
    """Ego lateral offset in meters. Positive is left."""
    ego_offset_up: float = 0.0
    """Ego vertical offset in meters."""
    ego_offset_video_scene_pkl: Optional[Path] = None
    """Override path to video_scene_dict.pkl for ego offset."""

    def main(self) -> None:
        def update_config(config: TrainerConfig) -> TrainerConfig:
            config.pipeline.datamanager.dataparser.train_split_fraction = 1.0
            config.pipeline.datamanager.dataparser.cameras = self.multi_view_camera
            CONSOLE.print(f"Using cams {self.multi_view_camera}.", style="bold green")
            return config

        config, pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,
            test_mode="inference",
            update_config_callback=update_config,
        )

        install_checks.check_ffmpeg_installed()

        if self.pose_source == "eval":
            assert pipeline.datamanager.eval_dataset is not None
            cameras = pipeline.datamanager.eval_dataset.cameras
            dataparser_outputs = pipeline.datamanager.eval_dataset._dataparser_outputs
        else:
            assert pipeline.datamanager.train_dataset is not None
            cameras = pipeline.datamanager.train_dataset.cameras
            dataparser_outputs = pipeline.datamanager.train_dataset._dataparser_outputs

        num_cameras = len(self.multi_view_camera)

        def ego_offset_active() -> bool:
            return any(
                value != 0.0 for value in (self.ego_offset_forward, self.ego_offset_lateral, self.ego_offset_up)
            )

        video_scene_pkl = None
        frame_info_map = None
        if self.retarget_vehicle_token is not None or ego_offset_active():
            override_pkl = self.retarget_video_scene_pkl or self.ego_offset_video_scene_pkl
            video_scene_pkl = _resolve_video_scene_pkl(config, override_pkl)
            if video_scene_pkl is None:
                raise ValueError("retarget/ego offset requires a valid video_scene_dict.pkl")

        def render_travel(
            output_dir: Path,
            cameras: Cameras,
            num_cameras: int,
            travel_id: Optional[int] = None,
            original_indices: Optional[List[int]] = None,
        ) -> None:
            cam_split = [cameras[i::num_cameras] for i in range(num_cameras)]
            if self.retarget_vehicle_token is not None or ego_offset_active():
                if self.interpolation_steps > 1:
                    raise ValueError("retarget/ego offset requires --interpolation-steps 1")
                assert travel_id is not None
                nonlocal frame_info_map
                if frame_info_map is None:
                    frame_info_map = _load_frame_info_map(video_scene_pkl, travel_id)

            rendered: Dict[str, List[np.ndarray]] = {}
            for cam_id, cam_group in enumerate(cam_split):
                cam_name = self.multi_view_camera[cam_id]
                per_camera_metadata = None
                if self.interpolation_steps <= 1:
                    camera_path = cam_group
                    seconds = len(cam_group) / self.frame_rate
                else:
                    seconds = (self.interpolation_steps * (len(cam_group) - 1) + 1) / self.frame_rate
                    camera_path = _get_interpolated_camera_path(
                        cameras=cam_group,
                        steps=self.interpolation_steps,
                        order_poses=self.order_poses,
                    )
                camera_path.camera_to_worlds = _apply_camera_offset_camera_coords(
                    camera_path.camera_to_worlds,
                    self.camera_offset_cam,
                )
                metadata: Dict[str, Any] = {"travel_id": travel_id}
                cam_frame_tokens = None
                cam_frame_ids = None
                if self.retarget_vehicle_token is not None or ego_offset_active():
                    frame_tokens = dataparser_outputs.frame_tokens
                    frame_ids = dataparser_outputs.frame_ids
                    if frame_tokens is None or frame_ids is None:
                        raise ValueError("retarget/ego offset requires frame_tokens and frame_ids in dataparser outputs.")
                    if original_indices is None:
                        cam_frame_tokens = frame_tokens[cam_id::num_cameras]
                        cam_frame_ids = frame_ids[cam_id::num_cameras]
                    else:
                        cam_indices = original_indices[cam_id::num_cameras]
                        cam_frame_tokens = [frame_tokens[idx] for idx in cam_indices]
                        cam_frame_ids = [frame_ids[idx] for idx in cam_indices]

                if ego_offset_active():
                    offset_local = np.array(
                        [self.ego_offset_forward, self.ego_offset_lateral, self.ego_offset_up], dtype=np.float32
                    )
                    offsets_world = []
                    for frame_token in cam_frame_tokens:
                        frame_info = frame_info_map.get(frame_token)
                        if frame_info is None:
                            offsets_world.append([0.0, 0.0, 0.0])
                            continue
                        e2g_rot = Quaternion(frame_info["ego2global_rotation"]).rotation_matrix
                        offsets_world.append(e2g_rot @ offset_local)
                    offsets_world = torch.tensor(np.array(offsets_world, dtype=np.float32))
                    camera_path.camera_to_worlds[:, :3, 3] = (
                        camera_path.camera_to_worlds[:, :3, 3] + offsets_world
                    )

                if self.retarget_vehicle_token is not None:
                    per_camera_metadata = []
                    yaw_offset = np.deg2rad(self.retarget_yaw_deg)
                    offset_local = np.array([self.retarget_forward, self.retarget_lateral, self.retarget_up], dtype=np.float32)
                    for frame_token, frame_id in zip(cam_frame_tokens, cam_frame_ids):
                        frame_info = frame_info_map.get(frame_token)
                        retarget_entry = {
                            "frame_idx": int(frame_id),
                            "retarget_vehicle_token": self.retarget_vehicle_token,
                            "retarget_mask": False,
                        }
                        if frame_info is None:
                            retarget_entry["retarget_trans"] = torch.zeros(3, dtype=torch.float32)
                            retarget_entry["retarget_quat"] = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                            per_camera_metadata.append(retarget_entry)
                            continue
                        active = True
                        if self.retarget_frame_start is not None and frame_id < self.retarget_frame_start:
                            active = False
                        if self.retarget_frame_end is not None and frame_id > self.retarget_frame_end:
                            active = False
                        e2g_trans = np.array(frame_info["ego2global_translation"], dtype=np.float32)
                        e2g_rot = Quaternion(frame_info["ego2global_rotation"]).rotation_matrix
                        e2g_yaw = Quaternion(frame_info["ego2global_rotation"]).yaw_pitch_roll[0]
                        target_trans = e2g_rot @ offset_local + e2g_trans
                        target_yaw = e2g_yaw + yaw_offset
                        target_quat = Quaternion(axis=[0, 0, 1], angle=target_yaw).q
                        retarget_entry["retarget_trans"] = torch.tensor(target_trans, dtype=torch.float32)
                        retarget_entry["retarget_quat"] = torch.tensor(target_quat, dtype=torch.float32)
                        retarget_entry["retarget_mask"] = active
                        per_camera_metadata.append(retarget_entry)
                camera_path.metadata = metadata
                output_filename = output_dir / f"{cam_name}.mp4"
                images = _render_trajectory_video(
                    pipeline,
                    camera_path,
                    output_filename=output_filename,
                    rendered_output_names=self.rendered_output_names,
                    rendered_resolution_scaling_factor=1.0 / self.downscale_factor,
                    seconds=seconds,
                    output_format="none",
                    image_format=self.image_format,
                    jpeg_quality=self.jpeg_quality,
                    depth_near_plane=self.depth_near_plane,
                    depth_far_plane=self.depth_far_plane,
                    colormap_options=self.colormap_options,
                    per_camera_metadata=per_camera_metadata,
                )
                rendered[cam_name] = images

            output_dir.mkdir(parents=True, exist_ok=True)
            output_filename = output_dir / self.output_name
            if output_filename.suffix == "":
                output_filename = output_filename.with_suffix(".mp4")

            composed = _compose_sticker_video(
                rendered=rendered,
                sticker_scale=self.sticker_scale,
                padding=self.padding,
                background_color=self.background_color,
            )
            media.write_video(output_filename, composed, fps=self.frame_rate)

            table = Table(
                title=None,
                show_header=False,
                box=box.MINIMAL,
                title_style=style.Style(bold=True),
            )
            table.add_row("Sticker Video", str(output_filename))
            CONSOLE.print(Panel(table, title="[bold][green]:tada: Sticker Render Complete :tada:[/bold]", expand=False))

        base_output_path = Path()
        if self.output_path is not None:
            base_output_path = self.output_path
        else:
            if hasattr(config, "base_dir"):
                base_output_path = Path(f"renders/scene_videos/{os.path.basename(config.base_dir)}")
            else:
                base_output_path = Path(f"renders/{config.experiment_name}")

        if hasattr(dataparser_outputs, "travel_ids") and dataparser_outputs.travel_ids is not None:
            travel_ids = dataparser_outputs.travel_ids
            travel_id_set = list(set(travel_ids))
            cameras_travels = {k: [] for k in travel_id_set}
            for idx, travel_id in enumerate(travel_ids):
                cameras_travels[travel_id].append(idx)
            for travel_id in travel_id_set:
                output_dir = base_output_path / f"travel_{travel_id}"
                travel_indices = torch.tensor(cameras_travels[travel_id], dtype=torch.int64)
                render_travel(
                    output_dir,
                    cameras[travel_indices],
                    num_cameras,
                    travel_id,
                    original_indices=travel_indices.tolist(),
                )
        else:
            render_travel(base_output_path, cameras, num_cameras)

@contextmanager
def _disable_datamanager_setup(cls):
    """
    Disables setup_train or setup_eval for faster initialization.
    """
    old_setup_train = getattr(cls, "setup_train")
    old_setup_eval = getattr(cls, "setup_eval")
    setattr(cls, "setup_train", lambda *args, **kwargs: None)
    setattr(cls, "setup_eval", lambda *args, **kwargs: None)
    yield cls
    setattr(cls, "setup_train", old_setup_train)
    setattr(cls, "setup_eval", old_setup_eval)


@dataclass
class DatasetRender(BaseRender):
    """Render all images in the dataset."""

    output_path: Path = Path("renders")
    """Path to output video file."""
    data: Optional[Path] = None
    """Override path to the dataset."""
    downscale_factor: Optional[float] = None
    """Scaling factor to apply to the camera image resolution."""
    split: Literal["train", "val", "test", "train+test"] = "test"
    """Split to render."""
    rendered_output_names: Optional[List[str]] = field(default_factory=lambda: None)
    """Name of the renderer outputs to use. rgb, depth, raw-depth, gt-rgb etc. By default all outputs are rendered."""

    def main(self):
        config: TrainerConfig

        def update_config(config: TrainerConfig) -> TrainerConfig:
            data_manager_config = config.pipeline.datamanager
            assert isinstance(data_manager_config, (VanillaDataManagerConfig, FullImageDatamanagerConfig))
            data_manager_config.eval_num_images_to_sample_from = -1
            data_manager_config.eval_num_times_to_repeat_images = -1
            if isinstance(data_manager_config, VanillaDataManagerConfig):
                data_manager_config.train_num_images_to_sample_from = -1
                data_manager_config.train_num_times_to_repeat_images = -1
            if self.data is not None:
                data_manager_config.data = self.data
            if self.downscale_factor is not None:
                assert hasattr(data_manager_config.dataparser, "downscale_factor")
                setattr(data_manager_config.dataparser, "downscale_factor", self.downscale_factor)
            return config

        config, pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,
            test_mode="inference",
            update_config_callback=update_config,
        )
        data_manager_config = config.pipeline.datamanager
        assert isinstance(data_manager_config, (VanillaDataManagerConfig, FullImageDatamanagerConfig))

        for split in self.split.split("+"):
            datamanager: VanillaDataManager
            dataset: Dataset
            if split == "train":
                with _disable_datamanager_setup(data_manager_config._target):  # pylint: disable=protected-access
                    datamanager = data_manager_config.setup(test_mode="test", device=pipeline.device)

                dataset = datamanager.train_dataset
                dataparser_outputs = getattr(dataset, "_dataparser_outputs", datamanager.train_dataparser_outputs)
            else:
                with _disable_datamanager_setup(data_manager_config._target):  # pylint: disable=protected-access
                    datamanager = data_manager_config.setup(test_mode=split, device=pipeline.device)

                dataset = datamanager.eval_dataset
                dataparser_outputs = getattr(dataset, "_dataparser_outputs", None)
                if dataparser_outputs is None:
                    dataparser_outputs = datamanager.dataparser.get_dataparser_outputs(split=datamanager.test_split)
            dataloader = FixedIndicesEvalDataloader(
                input_dataset=dataset,
                device=datamanager.device,
                num_workers=datamanager.world_size * 4,
            )
            images_root = Path(os.path.commonpath(dataparser_outputs.image_filenames))
            with Progress(
                TextColumn(f":movie_camera: Rendering split {split} :movie_camera:"),
                BarColumn(),
                TaskProgressColumn(
                    text_format="[progress.percentage]{task.completed}/{task.total:>.0f}({task.percentage:>3.1f}%)",
                    show_speed=True,
                ),
                ItersPerSecColumn(suffix="fps"),
                TimeRemainingColumn(elapsed_when_finished=False, compact=False),
                TimeElapsedColumn(),
            ) as progress:
                for camera_idx, (camera, batch) in enumerate(progress.track(dataloader, total=len(dataset))):
                    with torch.no_grad():
                        outputs = pipeline.model.get_outputs_for_camera(camera)

                    gt_batch = batch.copy()
                    gt_batch["rgb"] = gt_batch.pop("image")
                    all_outputs = (
                        list(outputs.keys())
                        + [f"raw-{x}" for x in outputs.keys()]
                        + [f"gt-{x}" for x in gt_batch.keys()]
                        + [f"raw-gt-{x}" for x in gt_batch.keys()]
                    )
                    rendered_output_names = self.rendered_output_names
                    if rendered_output_names is None:
                        rendered_output_names = ["gt-rgb"] + list(outputs.keys())
                    for rendered_output_name in rendered_output_names:
                        if rendered_output_name not in all_outputs:
                            CONSOLE.rule("Error", style="red")
                            CONSOLE.print(
                                f"Could not find {rendered_output_name} in the model outputs", justify="center"
                            )
                            CONSOLE.print(
                                f"Please set --rendered-output-name to one of: {all_outputs}", justify="center"
                            )
                            sys.exit(1)

                        is_raw = False
                        is_depth = rendered_output_name.find("depth") != -1
                        image_name = f"{camera_idx:05d}"

                        # Try to get the original filename
                        image_name = dataparser_outputs.image_filenames[camera_idx].relative_to(images_root)

                        output_path = self.output_path / split / rendered_output_name / image_name
                        output_path.parent.mkdir(exist_ok=True, parents=True)

                        output_name = rendered_output_name
                        if output_name.startswith("raw-"):
                            output_name = output_name[4:]
                            is_raw = True
                            if output_name.startswith("gt-"):
                                output_name = output_name[3:]
                                output_image = gt_batch[output_name]
                            else:
                                output_image = outputs[output_name]
                                if is_depth:
                                    # Divide by the dataparser scale factor
                                    output_image.div_(dataparser_outputs.dataparser_scale)
                        else:
                            if output_name.startswith("gt-"):
                                output_name = output_name[3:]
                                output_image = gt_batch[output_name]
                            else:
                                output_image = outputs[output_name]
                        del output_name

                        # Map to color spaces / numpy
                        if is_raw:
                            output_image = output_image.cpu().numpy()
                        elif is_depth:
                            output_image = (
                                colormaps.apply_depth_colormap(
                                    output_image,
                                    accumulation=outputs["accumulation"],
                                    near_plane=self.depth_near_plane,
                                    far_plane=self.depth_far_plane,
                                    colormap_options=self.colormap_options,
                                )
                                .cpu()
                                .numpy()
                            )
                        else:
                            output_image = (
                                colormaps.apply_colormap(
                                    image=output_image,
                                    colormap_options=self.colormap_options,
                                )
                                .cpu()
                                .numpy()
                            )

                        # Save to file
                        if is_raw:
                            with gzip.open(output_path.with_suffix(".npy.gz"), "wb") as f:
                                np.save(f, output_image)
                        elif self.image_format == "png":
                            media.write_image(output_path.with_suffix(".png"), output_image, fmt="png")
                        elif self.image_format == "jpeg":
                            media.write_image(
                                output_path.with_suffix(".jpg"), output_image, fmt="jpeg", quality=self.jpeg_quality
                            )
                        else:
                            raise ValueError(f"Unknown image format {self.image_format}")

        table = Table(
            title=None,
            show_header=False,
            box=box.MINIMAL,
            title_style=style.Style(bold=True),
        )
        for split in self.split.split("+"):
            table.add_row(f"Outputs {split}", str(self.output_path / split))
        CONSOLE.print(Panel(table, title="[bold][green]:tada: Render on split {} Complete :tada:[/bold]", expand=False))


Commands = tyro.conf.FlagConversionOff[
    Union[
        Annotated[RenderCameraPath, tyro.conf.subcommand(name="camera-path")],
        Annotated[RenderInterpolated, tyro.conf.subcommand(name="interpolate")],
        Annotated[RenderSticker, tyro.conf.subcommand(name="sticker")],
        Annotated[DatasetRender, tyro.conf.subcommand(name="dataset")],
    ]
]


def entrypoint():
    """Entrypoint for use with pyproject scripts."""
    tyro.extras.set_accent_color("bright_yellow")
    tyro.cli(Commands).main()


if __name__ == "__main__":
    entrypoint()


def get_parser_fn():
    """Get the parser function for the sphinx docs."""
    return tyro.extras.get_parser(Commands)  # noqa
